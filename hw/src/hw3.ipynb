{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2336782c-4ac0-483d-bb5d-672220c6035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from io import BytesIO\n",
    "# import pandas as pd\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import psycopg2\n",
    "\n",
    "#required for navigating machine's directory\n",
    "import glob\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "from scrapy import Selector\n",
    "import requests\n",
    "\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, FloatType, LongType, StringType, DoubleType, NumericType\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import VectorAssembler, Imputer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import when, col\n",
    "from itertools import combinations\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18eac28b-5d93-4a3a-b30b-3afc5dda624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../data\"\n",
    "\n",
    "NUMBER_OF_FOLDS = 3\n",
    "SPLIT_SEED = 7576\n",
    "TRAIN_TEST_SPLIT = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd295f28-eb11-445f-90ee-081db6e2941e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/23 01:33:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "__provides__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# data.show()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     spark\u001b[38;5;241m.\u001b[39mstop()\n\u001b[0;32m---> 17\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m data \u001b[38;5;241m=\u001b[39m clean_data(raw_data)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# data.show()\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# data.show()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "Cell \u001b[0;32mIn[11], line 35\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     18\u001b[0m features \u001b[38;5;241m=\u001b[39m [f\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mfields]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print(features)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# features_with_na = [col for col in features if data.filter(data[col].isNull()).count() != 0]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#     outputCol=\"features\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m assembler \u001b[38;5;241m=\u001b[39m \u001b[43mVectorAssembler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputCols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Define a Random Forest classifier\u001b[39;00m\n\u001b[1;32m     41\u001b[0m classifier \u001b[38;5;241m=\u001b[39m RandomForestClassifier(labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m, featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/feature.py:5357\u001b[0m, in \u001b[0;36mVectorAssembler.__init__\u001b[0;34m(self, inputCols, outputCol, handleInvalid)\u001b[0m\n\u001b[1;32m   5346\u001b[0m \u001b[38;5;129m@keyword_only\u001b[39m\n\u001b[1;32m   5347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   5348\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5352\u001b[0m     handleInvalid: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5353\u001b[0m ):\n\u001b[1;32m   5354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5355\u001b[0m \u001b[38;5;124;03m    __init__(self, \\\\*, inputCols=None, outputCol=None, handleInvalid=\"error\")\u001b[39;00m\n\u001b[1;32m   5356\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5357\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mVectorAssembler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_java_obj(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.ml.feature.VectorAssembler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid)\n\u001b[1;32m   5359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(handleInvalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/wrapper.py:49\u001b[0m, in \u001b[0;36mJavaWrapper.__init__\u001b[0;34m(self, java_obj)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, java_obj: Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mJavaWrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m java_obj\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:219\u001b[0m, in \u001b[0;36mHasInputCols.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasInputCols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:241\u001b[0m, in \u001b[0;36mHasOutputCol.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasOutputCol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:421\u001b[0m, in \u001b[0;36mHasHandleInvalid.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasHandleInvalid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/__init__.py:269\u001b[0m, in \u001b[0;36mParams.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params: Optional[List[Param]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Copy the params from the class to the object\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_copy_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/__init__.py:276\u001b[0m, in \u001b[0;36mParams._copy_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03mCopy all params defined on the class to current object.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 276\u001b[0m src_name_attrs \u001b[38;5;241m=\u001b[39m [(x, \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[38;5;28mcls\u001b[39m)]\n\u001b[1;32m    277\u001b[0m src_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m nameAttr: \u001b[38;5;28misinstance\u001b[39m(nameAttr[\u001b[38;5;241m1\u001b[39m], Param), src_name_attrs))\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m src_params:\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/__init__.py:276\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03mCopy all params defined on the class to current object.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 276\u001b[0m src_name_attrs \u001b[38;5;241m=\u001b[39m [(x, \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[38;5;28mcls\u001b[39m)]\n\u001b[1;32m    277\u001b[0m src_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m nameAttr: \u001b[38;5;28misinstance\u001b[39m(nameAttr[\u001b[38;5;241m1\u001b[39m], Param), src_name_attrs))\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m src_params:\n",
      "\u001b[0;31mAttributeError\u001b[0m: __provides__"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Create a Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Predict Heart Disease\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    raw_data = read_data(spark)\n",
    "    data = clean_data(raw_data)\n",
    "    # data.show()\n",
    "    pipeline(data)\n",
    "    \n",
    "    # data.show()\n",
    "\n",
    "\n",
    "    spark.stop()\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c08257-985e-4e91-9402-84f1c603a19e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Get data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ef127-ab45-4266-989d-3b767f22583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3',\n",
    "                  aws_access_key_id='',\n",
    "                  aws_secret_access_key='',\n",
    "                  aws_session_token='')\n",
    "\n",
    "\n",
    "bucket_name = 'de300spring2024'\n",
    "object_key = 'emily_kohlberg/hw/heart_disease.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ede6f-9c79-4732-b2b3-02187802c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_obj = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee58c7b-d178-4d1a-9404-53f63ad3d5ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(BytesIO(csv_string.encode()))\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8437dc-a484-4e3c-9325-fc6e97d76487",
   "metadata": {},
   "source": [
    "## Get data from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d7aff3c-c4c5-4f70-a8cc-906d79c091c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(spark: SparkSession) -> DataFrame:\n",
    "    \"\"\"\n",
    "    read data; since the data has the header we let spark guess the schema\n",
    "    \"\"\"\n",
    "    \n",
    "    data = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(os.path.join(DATA_FOLDER,\"*.csv\"))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdc3cc5-fc10-49c1-ac72-3b45c4798c04",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d613c02b-84f6-4e60-b556-6b8146af89c7",
   "metadata": {},
   "source": [
    "### Clean and Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaab5a38-1e58-404e-aa5c-2803604c4873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_cols(data: DataFrame) -> DataFrame:\n",
    "    columns_to_retain = ['age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', 'smoke', \n",
    "                         'fbs', 'prop', 'nitr', 'pro', 'diuretic', 'thaldur', 'thalach', \n",
    "                         'exang', 'oldpeak', 'slope', 'target']\n",
    "    \n",
    "    filtered_data = data.select(columns_to_retain)\n",
    "    return filtered_data\n",
    "    \n",
    "def replace_out_of_range(data: DataFrame) -> DataFrame:\n",
    "    data = data.withColumn('painloc', when(col('painloc') < 0, 0).when(col('painloc') > 1, 1).otherwise(col('painloc')))\n",
    "    data = data.withColumn('painexer', when(col('painexer') < 0, 0).when(col('painexer') > 1, 1).otherwise(col('painexer')))\n",
    "    data = data.withColumn('trestbps', when(col('trestbps') < 100, 100).otherwise(col('trestbps')))\n",
    "    data = data.withColumn('oldpeak', when(col('oldpeak') < 0, 0).when(col('oldpeak') > 4, 4).otherwise(col('oldpeak')))\n",
    "    data = data.withColumn('fbs', when(col('fbs') < 0, 0).when(col('fbs') > 1, 1).otherwise(col('fbs')))\n",
    "    data = data.withColumn('prop', when(col('prop') < 0, 0).when(col('prop') > 1, 1).otherwise(col('prop')))\n",
    "    data = data.withColumn('nitr', when(col('nitr') < 0, 0).when(col('nitr') > 1, 1).otherwise(col('nitr')))\n",
    "    data = data.withColumn('pro', when(col('pro') < 0, 0).when(col('pro') > 1, 1).otherwise(col('pro')))\n",
    "    data = data.withColumn('diuretic', when(col('diuretic') < 0, 0).when(col('diuretic') > 1, 1).otherwise(col('diuretic')))\n",
    "    data = data.withColumn('exang', when(col('exang') < 0, 0).when(col('exang') > 1, 1).otherwise(col('exang')))\n",
    "    data = data.withColumn('slope', when(col('slope') < 1, None).when(col('slope') > 3, None).otherwise(col('slope')))\n",
    "    return data\n",
    "    \n",
    "def replace_nulls_with_mean(data: DataFrame) -> DataFrame:\n",
    "    columns_for_imputation = ['age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', \n",
    "                     'fbs', 'prop', 'nitr', 'pro', 'diuretic', 'thaldur', 'thalach', \n",
    "                     'exang', 'oldpeak', 'slope', 'target']\n",
    "    \n",
    "    for column in columns_for_imputation:\n",
    "        mean_value = data.select(F.mean(col(column))).collect()[0][0]\n",
    "        if mean_value is not None:\n",
    "            data = data.withColumn(column, when(col(column).isNull(), mean_value).otherwise(col(column)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2876ad52-7dd2-41a8-a252-8b7f778f07c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoke_1(data: DataFrame) -> DataFrame:\n",
    "    url1 = 'https://www.abs.gov.au/statistics/health/health-conditions-and-risks/smoking-and-vaping/latest-release'\n",
    "    response = requests.get(url1)\n",
    "        \n",
    "    # get the HTML file as a string\n",
    "    html_content = response.content\n",
    "    \n",
    "    # create a selector object\n",
    "    full_sel = Selector(text=html_content)\n",
    "    \n",
    "    # select all tables in page -> returns a SelectorList object\n",
    "    tables = full_sel.xpath('//table')\n",
    "    smokers_by_age = tables[1]\n",
    "    # get the rows\n",
    "    rows = smokers_by_age.xpath('./tbody//tr')\n",
    "\n",
    "    def parse_row_1(row:Selector) -> List[str]:\n",
    "        '''\n",
    "        Parses a html row into a list of individual elements\n",
    "        '''\n",
    "        cells = row.xpath('.//th | .//td')\n",
    "        row_data = []\n",
    "        \n",
    "        for i, cell in enumerate(cells):\n",
    "            if i == 0 or i == 10:\n",
    "                cell_text = cell.xpath('normalize-space(.)').get()\n",
    "                cell_text = re.sub(r'<.*?>', ' ', cell_text)  # Remove remaining HTML tags\n",
    "                # if there are br tags, there will be some binary characters\n",
    "                cell_text = cell_text.replace('\\xa0', '')  # Remove \\xa0 characters\n",
    "                row_data.append(cell_text)\n",
    "        \n",
    "        return row_data\n",
    "    \n",
    "    table_data = [parse_row_1(row) for row in rows]\n",
    "\n",
    "    def get_rate_1(age):\n",
    "        try:\n",
    "            age = int(age)\n",
    "            for i, row in enumerate(table_data):\n",
    "                if i < len(table_data) - 1:\n",
    "                    cutoff = row[0].split('–')[1]\n",
    "                    if age <= int(cutoff):\n",
    "                        return float(row[1])\n",
    "                else:\n",
    "                    return float(row[1])\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    # Register the UDF\n",
    "    get_rate_1_udf = F.udf(lambda age: get_rate_1(age) / 100, DoubleType())\n",
    "\n",
    "    data = data.withColumn('smoke_1', when(col('smoke_1').isNull(), get_rate_1_udf(col('age'))).otherwise(col('smoke_1')))\n",
    "\n",
    "    return data\n",
    "\n",
    "def smoke_2(data: DataFrame) -> DataFrame:\n",
    "    url2 = 'https://www.cdc.gov/tobacco/data_statistics/fact_sheets/adult_data/cig_smoking/index.htm'\n",
    "    response = requests.get(url2)\n",
    "\n",
    "    # Create a scrapy Selector from the response content\n",
    "    selector = Selector(text=response.content)\n",
    "\n",
    "    ul_sel_list = selector.xpath('//ul[@class=\"block-list\"]')\n",
    "    genders = ul_sel_list[0]\n",
    "    ages = ul_sel_list[1]\n",
    "\n",
    "    def clean_gender_percents(rows):\n",
    "        dict = {}\n",
    "        for row in rows:\n",
    "            gender = 'woman' if 'women' in row.split('(')[0] else 'man'\n",
    "            percent = float(row.split('(')[1].split('%')[0])\n",
    "            dict[gender] = float(percent)\n",
    "        return dict\n",
    "\n",
    "    def clean_age_percents(rows):\n",
    "        for i, row in enumerate(rows):\n",
    "            if i < len(rows) - 1:\n",
    "                age = int(row.split('–')[1].split(' ')[0])\n",
    "            else:\n",
    "                age = int(row.split(' ')[7])\n",
    "                \n",
    "            percent = float(row.split('(')[1].split('%')[0])\n",
    "            rows[i] = [age, percent]\n",
    "        return rows\n",
    "\n",
    "    def parse_row_2(row:Selector) -> List[str]:\n",
    "        '''\n",
    "        Parses a html row into a list of individual elements\n",
    "        '''\n",
    "        cells = row.xpath('./li')\n",
    "        row_data = []\n",
    "        \n",
    "        for i, cell in enumerate(cells):\n",
    "            cell_text = cell.xpath('normalize-space(.)').get()\n",
    "            cell_text = re.sub(r'<.*?>', ' ', cell_text)  # Remove remaining HTML tags\n",
    "            # if there are br tags, there will be some binary characters\n",
    "            cell_text = cell_text.replace('\\xa0', '')  # Remove \\xa0 characters\n",
    "            row_data.append(cell_text)\n",
    "        \n",
    "        return row_data\n",
    "\n",
    "    per_by_gender = clean_gender_percents(parse_row_2(genders))\n",
    "    per_by_age = clean_age_percents(parse_row_2(ages))\n",
    "\n",
    "    def get_rate_2(sex, age):\n",
    "        if sex == 0:\n",
    "            try:\n",
    "                age = int(age)\n",
    "                for i, row in enumerate(per_by_age):\n",
    "                    if i < len(per_by_age) - 1:\n",
    "                        if age <= row[0]:\n",
    "                            return row[1]\n",
    "                    else:\n",
    "                        return row[1]\n",
    "            except:\n",
    "                return np.nan\n",
    "        else:\n",
    "            try:\n",
    "                age = int(age)\n",
    "                for i, row in enumerate(per_by_age):\n",
    "                    if i < len(per_by_age) - 1:\n",
    "                        if age <= row[0]:\n",
    "                            return row[1] * per_by_gender['man'] / per_by_gender['woman']\n",
    "                    else:\n",
    "                        return row[1] * per_by_gender['man'] / per_by_gender['woman']\n",
    "            except:\n",
    "                return np.nan\n",
    "\n",
    "    # Register the UDF\n",
    "    get_rate_2_udf = F.udf(lambda sex, age: get_rate_2(sex, age) / 100, DoubleType())\n",
    "\n",
    "    data = data.withColumn('smoke_2', when(col('smoke_2').isNull(), get_rate_2_udf(col('sex'), col('age'))).otherwise(col('smoke_2')))\n",
    "\n",
    "    return data \n",
    "\n",
    "def impute_smoke(data: DataFrame) -> DataFrame:\n",
    "    data = data.withColumn('smoke_1', F.col('smoke'))\n",
    "    data = data.withColumn('smoke_2', F.col('smoke'))\n",
    "\n",
    "    data = smoke_1(data)\n",
    "    data = smoke_2(data)\n",
    "\n",
    "    data = data.drop('smoke')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344c3db1-7e68-4f9a-84ba-1504017673af",
   "metadata": {},
   "source": [
    "### Final Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "650095cd-48a6-4fd4-9f74-5d01e35f9ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data: DataFrame) -> DataFrame:\n",
    "    data = retain_cols(data)\n",
    "    data = replace_out_of_range(data)\n",
    "    data = replace_nulls_with_mean(data)\n",
    "    data = impute_smoke(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d54632-f481-4f72-bc1d-c9a66a1fb582",
   "metadata": {},
   "source": [
    "## Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83af3760-edd2-4965-b655-34539851c644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(data: DataFrame):\n",
    "\n",
    "    # drop null targets\n",
    "    data = data.dropna(subset=['target'])\n",
    "\n",
    "    # make age an int\n",
    "    data = data.withColumn(\"age\", data[\"age\"].cast(IntegerType()))\n",
    "\n",
    "    # numeric_features = [f.name for f in data.schema.fields if isinstance(f.dataType, NumericType)]\n",
    "\n",
    "\n",
    "    # # numeric_features = [f.name for f in data.schema.fields if isinstance(f.dataType, DoubleType) or isinstance(f.dataType, FloatType) or isinstance(f.dataType, IntegerType) or isinstance(f.dataType, LongType)]\n",
    "    # # string_features = [f.name for f in data.schema.fields if isinstance(f.dataType, StringType)]\n",
    "\n",
    "    # # print(numeric_features)\n",
    "    # # print(string_features)\n",
    "\n",
    "    features = [f.name for f in data.schema.fields]\n",
    "    # print(features)\n",
    "\n",
    "    # features_with_na = [col for col in features if data.filter(data[col].isNull()).count() != 0]\n",
    "    # print(features_with_na)\n",
    "\n",
    "\n",
    "    # imputed_columns = [f\"Imputed{v}\" for v in numeric_features]\n",
    "    \n",
    "    # imputer = Imputer(inputCols=numeric_features, outputCols=imputed_columns, strategy=\"mean\")\n",
    "\n",
    "\n",
    "    # Assemble feature columns into a single feature vector\n",
    "    # assembler = VectorAssembler(\n",
    "    #     inputCols=imputed_columns, \n",
    "    #     outputCol=\"features\"\n",
    "    #     )\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=features, \n",
    "        outputCol=\"features\"\n",
    "        )\n",
    "\n",
    "    # Define a Random Forest classifier\n",
    "    classifier = RandomForestClassifier(labelCol=\"target\", featuresCol=\"features\")\n",
    "\n",
    "    # Create the pipeline\n",
    "    # pipeline = Pipeline(stages=[imputer, assembler, classifier])\n",
    "    pipeline = Pipeline(stages=[assembler, classifier])\n",
    "    \n",
    "    # Set up the parameter grid for maximum tree depth\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(classifier.maxDepth, [2, 4, 6, 8, 10]) \\\n",
    "        .addGrid(classifier.numTrees, [150, 200, 250, 500]) \\\n",
    "        .build()\n",
    "\n",
    "    # Set up the cross-validator\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"target\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "    crossval = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=NUMBER_OF_FOLDS,\n",
    "        seed=SPLIT_SEED)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    train_data, test_data = data.randomSplit([TRAIN_TEST_SPLIT, 1-TRAIN_TEST_SPLIT], seed=SPLIT_SEED)\n",
    "\n",
    "    # Train the cross-validated pipeline model\n",
    "    cvModel = crossval.fit(train_data)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    predictions = cvModel.transform(test_data)\n",
    "\n",
    "    # Evaluate the model\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    print(f\"Area Under ROC Curve: {auc:.4f}\")\n",
    "\n",
    "    # Get the best RandomForest model\n",
    "    best_model = cvModel.bestModel.stages[-1]\n",
    "\n",
    "    # Retrieve the selected maximum tree depth\n",
    "    selected_max_depth = best_model.getOrDefault(best_model.getParam(\"maxDepth\"))\n",
    "\n",
    "    # Print the selected maximum tree depth\n",
    "    print(f\"Selected Maximum Tree Depth: {selected_max_depth}\")\n",
    "\n",
    "    # Retrieve the selected number of trees\n",
    "    selected_num_trees = best_model.getOrDefault(best_model.getParam(\"numTrees\"))\n",
    "\n",
    "    # Print the selected number of trees\n",
    "    print(f\"Selected Number of Trees: {selected_num_trees}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
