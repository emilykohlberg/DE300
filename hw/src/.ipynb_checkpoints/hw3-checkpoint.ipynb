{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2336782c-4ac0-483d-bb5d-672220c6035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import IntegerType, FloatType, LongType, StringType, DoubleType\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, Imputer\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, GBTClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator, Evaluator\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import when, col\n",
    "from itertools import combinations\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18eac28b-5d93-4a3a-b30b-3afc5dda624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../data\"\n",
    "\n",
    "NUMBER_OF_FOLDS = 5\n",
    "SPLIT_SEED = 7576\n",
    "TRAIN_TEST_SPLIT = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16a54f36-628a-47da-b1d5-864ff840c2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/demos/bin/python3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8437dc-a484-4e3c-9325-fc6e97d76487",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d7aff3c-c4c5-4f70-a8cc-906d79c091c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(spark: SparkSession) -> DataFrame:\n",
    "    \"\"\"\n",
    "    read data; since the data has the header we let spark guess the schema\n",
    "    \"\"\"\n",
    "    \n",
    "    data = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(os.path.join(DATA_FOLDER,\"heart_disease.csv\"))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdc3cc5-fc10-49c1-ac72-3b45c4798c04",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaab5a38-1e58-404e-aa5c-2803604c4873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_cols(data: DataFrame) -> DataFrame:\n",
    "    columns_to_retain = ['age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', 'smoke', \n",
    "                         'fbs', 'prop', 'nitr', 'pro', 'diuretic', 'thaldur', 'thalach', \n",
    "                         'exang', 'oldpeak', 'slope', 'target']\n",
    "    \n",
    "    filtered_data = data.select(columns_to_retain)\n",
    "    return filtered_data\n",
    "    \n",
    "def replace_out_of_range(data: DataFrame) -> DataFrame:\n",
    "    data = data.withColumn('painloc', when(col('painloc') < 0, 0).when(col('painloc') > 1, 1).otherwise(col('painloc')))\n",
    "    data = data.withColumn('painexer', when(col('painexer') < 0, 0).when(col('painexer') > 1, 1).otherwise(col('painexer')))\n",
    "    data = data.withColumn('trestbps', when(col('trestbps') < 100, 100).otherwise(col('trestbps')))\n",
    "    data = data.withColumn('oldpeak', when(col('oldpeak') < 0, 0).when(col('oldpeak') > 4, 4).otherwise(col('oldpeak')))\n",
    "    data = data.withColumn('fbs', when(col('fbs') < 0, 0).when(col('fbs') > 1, 1).otherwise(col('fbs')))\n",
    "    data = data.withColumn('prop', when(col('prop') < 0, 0).when(col('prop') > 1, 1).otherwise(col('prop')))\n",
    "    data = data.withColumn('nitr', when(col('nitr') < 0, 0).when(col('nitr') > 1, 1).otherwise(col('nitr')))\n",
    "    data = data.withColumn('pro', when(col('pro') < 0, 0).when(col('pro') > 1, 1).otherwise(col('pro')))\n",
    "    data = data.withColumn('diuretic', when(col('diuretic') < 0, 0).when(col('diuretic') > 1, 1).otherwise(col('diuretic')))\n",
    "    data = data.withColumn('exang', when(col('exang') < 0, 0).when(col('exang') > 1, 1).otherwise(col('exang')))\n",
    "    data = data.withColumn('slope', when(col('slope') < 1, None).when(col('slope') > 3, None).otherwise(col('slope')))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2876ad52-7dd2-41a8-a252-8b7f778f07c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data: DataFrame) -> DataFrame:\n",
    "    # clean\n",
    "    data = retain_cols(data)\n",
    "    data = replace_out_of_range(data)\n",
    "\n",
    "    # drop null targets\n",
    "    data = data.dropna(subset=[\"target\"])\n",
    "\n",
    "    # make age an int\n",
    "    data = data.withColumn(\"age\", data[\"age\"].cast(IntegerType()))\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d54632-f481-4f72-bc1d-c9a66a1fb582",
   "metadata": {},
   "source": [
    "## Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce311d12-501d-46f3-8fbd-0ddcb4315d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CompositeEvaluator\n",
    "class CompositeEvaluator(Evaluator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.auc_evaluator = BinaryClassificationEvaluator(labelCol=\"target\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "        self.accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "        self.precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "        self.recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "        self.f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "        self.weights = {\n",
    "            \"AUC\": 0.6,\n",
    "            \"accuracy\": 0.1,\n",
    "            \"precision\": 0.1,\n",
    "            \"recall\": 0.1,\n",
    "            \"f1\": 0.1\n",
    "        }\n",
    "\n",
    "    def isLargerBetter(self):\n",
    "        return True\n",
    "\n",
    "    def _evaluate(self, dataset):\n",
    "        auc = self.auc_evaluator.evaluate(dataset)\n",
    "        accuracy = self.accuracy_evaluator.evaluate(dataset)\n",
    "        precision = self.precision_evaluator.evaluate(dataset)\n",
    "        recall = self.recall_evaluator.evaluate(dataset)\n",
    "        f1 = self.f1_evaluator.evaluate(dataset)\n",
    "\n",
    "        composite_score = (self.weights[\"AUC\"] * auc +\n",
    "                           self.weights[\"accuracy\"] * accuracy +\n",
    "                           self.weights[\"precision\"] * precision +\n",
    "                           self.weights[\"recall\"] * recall +\n",
    "                           self.weights[\"f1\"] * f1)\n",
    "\n",
    "        return composite_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83af3760-edd2-4965-b655-34539851c644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(data: DataFrame):\n",
    "\n",
    "    numeric_features = [f.name for f in data.schema.fields if isinstance(f.dataType, (DoubleType, FloatType, IntegerType, LongType))]\n",
    "    numeric_features.remove(\"target\")\n",
    "\n",
    "    # numeric columns\n",
    "    imputed_columns = [f\"Imputed{v}\" for v in numeric_features]\n",
    "    imputer_numeric = Imputer(inputCols=numeric_features, outputCols=imputed_columns, strategy = \"mean\")\n",
    "    \n",
    "    # Assemble feature columns into a single feature vector\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=imputed_columns, \n",
    "        outputCol=\"features\"\n",
    "        )\n",
    "\n",
    "    # Define binary classification models\n",
    "    classifiers = {\n",
    "        \"RandomForest\": RandomForestClassifier(labelCol=\"target\", featuresCol=\"features\"),\n",
    "        \"LogisticRegression\": LogisticRegression(labelCol=\"target\", featuresCol=\"features\"),\n",
    "        \"GBTClassifier\": GBTClassifier(labelCol=\"target\", featuresCol=\"features\"),\n",
    "        \"DecisionTree\": DecisionTreeClassifier(labelCol=\"target\", featuresCol=\"features\")\n",
    "\n",
    "    }\n",
    "\n",
    "    # Define parameter grids for each classifier\n",
    "    param_grids = {\n",
    "        \"RandomForest\": ParamGridBuilder() \\\n",
    "            .addGrid(classifiers[\"RandomForest\"].maxDepth, [4, 6, 8]) \\\n",
    "            .addGrid(classifiers[\"RandomForest\"].numTrees, [50, 100]) \\\n",
    "            .build(),\n",
    "        \"LogisticRegression\": ParamGridBuilder() \\\n",
    "            .addGrid(classifiers[\"LogisticRegression\"].regParam, [0.01, 0.1]) \\\n",
    "            .build(),\n",
    "        \"GBTClassifier\": ParamGridBuilder() \\\n",
    "            .addGrid(classifiers[\"GBTClassifier\"].maxDepth, [2, 4]) \\\n",
    "            .addGrid(classifiers[\"GBTClassifier\"].maxIter, [10, 20]) \\\n",
    "            .build(),\n",
    "        \"DecisionTree\": ParamGridBuilder() \\\n",
    "            .addGrid(classifiers[\"DecisionTree\"].maxDepth, [4, 6, 8]) \\\n",
    "            .addGrid(classifiers[\"DecisionTree\"].minInstancesPerNode, [1, 2, 4]) \\\n",
    "            .build()\n",
    "    }\n",
    "    \n",
    "    # Set up the composite evaluator\n",
    "    composite_evaluator = CompositeEvaluator()\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    train_data, test_data = data.randomSplit([TRAIN_TEST_SPLIT, 1-TRAIN_TEST_SPLIT], seed=SPLIT_SEED)\n",
    "\n",
    "    # Cache the training data to improve performance\n",
    "    train_data.cache()\n",
    "\n",
    "    best_model = None\n",
    "    best_model_name = \"\"\n",
    "    best_composite_score = float('-inf')\n",
    "    \n",
    "    # Iterate through each classifier\n",
    "    for model_name, classifier in classifiers.items():\n",
    "        # Create the pipeline\n",
    "        pipeline = Pipeline(stages=[imputer_numeric, assembler, classifier])  \n",
    "\n",
    "        # Set up the cross-validator\n",
    "        crossval = CrossValidator(\n",
    "            estimator=pipeline,\n",
    "            estimatorParamMaps=param_grids[model_name],\n",
    "            evaluator=composite_evaluator,\n",
    "            numFolds=NUMBER_OF_FOLDS,\n",
    "            seed=SPLIT_SEED)\n",
    "\n",
    "        # Train the cross-validated pipeline model\n",
    "        cvModel = crossval.fit(train_data)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        predictions = cvModel.transform(test_data)\n",
    "\n",
    "        # Evaluate the model using the composite score\n",
    "        composite_score = composite_evaluator.evaluate(predictions)\n",
    "        print(f\"{model_name} Composite Score: {composite_score:.4f}\")\n",
    "\n",
    "        metrics = {\n",
    "                \"composite_score\": composite_score,\n",
    "                \"AUC\": composite_evaluator.auc_evaluator.evaluate(predictions),\n",
    "                \"accuracy\": composite_evaluator.accuracy_evaluator.evaluate(predictions),\n",
    "                \"precision\": composite_evaluator.precision_evaluator.evaluate(predictions),\n",
    "                \"recall\": composite_evaluator.recall_evaluator.evaluate(predictions),\n",
    "                \"f1\": composite_evaluator.f1_evaluator.evaluate(predictions)\n",
    "            }\n",
    "        print(f\"Metrics - AUC: {metrics['AUC']:.4f}, Accuracy: {metrics['accuracy']:.4f}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1']:.4f}\")\n",
    "\n",
    "\n",
    "        # Update the best model if current model is better\n",
    "        if composite_score > best_composite_score:\n",
    "            best_composite_score = composite_score\n",
    "            best_model = cvModel.bestModel.stages[-1]\n",
    "            best_model_name = model_name\n",
    "            best_metrics = {\n",
    "                \"composite_score\": composite_score,\n",
    "                \"AUC\": composite_evaluator.auc_evaluator.evaluate(predictions),\n",
    "                \"accuracy\": composite_evaluator.accuracy_evaluator.evaluate(predictions),\n",
    "                \"precision\": composite_evaluator.precision_evaluator.evaluate(predictions),\n",
    "                \"recall\": composite_evaluator.recall_evaluator.evaluate(predictions),\n",
    "                \"f1\": composite_evaluator.f1_evaluator.evaluate(predictions)\n",
    "            }\n",
    "            \n",
    "    # Print the best model information\n",
    "    print(f\"Best Model: {best_model_name}\")\n",
    "    print(f\"Best Model Composite Score: {best_composite_score:.4f}\")\n",
    "    print(f\"Best Model Metrics - AUC: {best_metrics['AUC']:.4f}, Accuracy: {best_metrics['accuracy']:.4f}, Precision: {best_metrics['precision']:.4f}, Recall: {best_metrics['recall']:.4f}, F1-Score: {best_metrics['f1']:.4f}\")\n",
    "\n",
    "\n",
    "    # Retrieve and print the best model parameters\n",
    "    if best_model_name == \"RandomForest\":\n",
    "        selected_max_depth = best_model.getOrDefault(best_model.getParam(\"maxDepth\"))\n",
    "        selected_num_trees = best_model.getOrDefault(best_model.getParam(\"numTrees\"))\n",
    "        print(f\"Selected Maximum Tree Depth: {selected_max_depth}\")\n",
    "        print(f\"Selected Number of Trees: {selected_num_trees}\")\n",
    "    elif best_model_name == \"LogisticRegression\":\n",
    "        selected_reg_param = best_model.getOrDefault(best_model.getParam(\"regParam\"))\n",
    "        print(f\"Selected Regularization Parameter: {selected_reg_param}\")\n",
    "    elif best_model_name == \"GBTClassifier\":\n",
    "        selected_max_depth = best_model.getOrDefault(best_model.getParam(\"maxDepth\"))\n",
    "        selected_max_iter = best_model.getOrDefault(best_model.getParam(\"maxIter\"))\n",
    "        print(f\"Selected Maximum Tree Depth: {selected_max_depth}\")\n",
    "        print(f\"Selected Maximum Iterations: {selected_max_iter}\")\n",
    "    elif best_model_name == \"DecisionTree\":\n",
    "        selected_max_depth = best_model.getOrDefault(best_model.getParam(\"maxDepth\"))\n",
    "        selected_min_instances_per_node = best_model.getOrDefault(best_model.getParam(\"minInstancesPerNode\"))\n",
    "        print(f\"Selected Maximum Tree Depth: {selected_max_depth}\")\n",
    "        print(f\"Selected Minimum Instances Per Node: {selected_min_instances_per_node}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd295f28-eb11-445f-90ee-081db6e2941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Create a Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Predict Heart Disease\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    data = read_data(spark)\n",
    "    data = clean_data(data)\n",
    "   \n",
    "    pipeline(data)\n",
    "\n",
    "    spark.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e95c1a82-6bdf-4871-9706-5c464430701a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/28 01:50:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/28 01:50:58 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/05/28 01:51:43 WARN DAGScheduler: Broadcasting large task binary with size 1199.1 KiB\n",
      "24/05/28 01:51:43 WARN DAGScheduler: Broadcasting large task binary with size 1585.1 KiB\n",
      "24/05/28 01:51:45 WARN DAGScheduler: Broadcasting large task binary with size 1246.1 KiB\n",
      "24/05/28 01:51:45 WARN DAGScheduler: Broadcasting large task binary with size 1258.7 KiB\n",
      "24/05/28 01:51:46 WARN DAGScheduler: Broadcasting large task binary with size 1258.7 KiB\n",
      "24/05/28 01:51:46 WARN DAGScheduler: Broadcasting large task binary with size 1258.7 KiB\n",
      "24/05/28 01:51:47 WARN DAGScheduler: Broadcasting large task binary with size 1258.7 KiB\n",
      "24/05/28 01:52:12 WARN DAGScheduler: Broadcasting large task binary with size 1236.9 KiB\n",
      "24/05/28 01:52:13 WARN DAGScheduler: Broadcasting large task binary with size 1642.7 KiB\n",
      "24/05/28 01:52:14 WARN DAGScheduler: Broadcasting large task binary with size 1286.2 KiB\n",
      "24/05/28 01:52:14 WARN DAGScheduler: Broadcasting large task binary with size 1298.7 KiB\n",
      "24/05/28 01:52:15 WARN DAGScheduler: Broadcasting large task binary with size 1298.7 KiB\n",
      "24/05/28 01:52:15 WARN DAGScheduler: Broadcasting large task binary with size 1298.7 KiB\n",
      "24/05/28 01:52:15 WARN DAGScheduler: Broadcasting large task binary with size 1298.7 KiB\n",
      "24/05/28 01:52:37 WARN DAGScheduler: Broadcasting large task binary with size 1213.3 KiB\n",
      "24/05/28 01:52:38 WARN DAGScheduler: Broadcasting large task binary with size 1601.2 KiB\n",
      "24/05/28 01:52:39 WARN DAGScheduler: Broadcasting large task binary with size 1256.6 KiB\n",
      "24/05/28 01:52:39 WARN DAGScheduler: Broadcasting large task binary with size 1269.1 KiB\n",
      "24/05/28 01:52:39 WARN DAGScheduler: Broadcasting large task binary with size 1269.1 KiB\n",
      "24/05/28 01:52:40 WARN DAGScheduler: Broadcasting large task binary with size 1269.1 KiB\n",
      "24/05/28 01:52:40 WARN DAGScheduler: Broadcasting large task binary with size 1269.1 KiB\n",
      "24/05/28 01:52:59 WARN DAGScheduler: Broadcasting large task binary with size 1206.1 KiB\n",
      "24/05/28 01:52:59 WARN DAGScheduler: Broadcasting large task binary with size 1590.1 KiB\n",
      "24/05/28 01:53:00 WARN DAGScheduler: Broadcasting large task binary with size 1258.4 KiB\n",
      "24/05/28 01:53:00 WARN DAGScheduler: Broadcasting large task binary with size 1270.9 KiB\n",
      "24/05/28 01:53:01 WARN DAGScheduler: Broadcasting large task binary with size 1270.9 KiB\n",
      "24/05/28 01:53:01 WARN DAGScheduler: Broadcasting large task binary with size 1270.9 KiB\n",
      "24/05/28 01:53:01 WARN DAGScheduler: Broadcasting large task binary with size 1270.9 KiB\n",
      "24/05/28 01:53:19 WARN DAGScheduler: Broadcasting large task binary with size 1216.5 KiB\n",
      "24/05/28 01:53:19 WARN DAGScheduler: Broadcasting large task binary with size 1625.6 KiB\n",
      "24/05/28 01:53:20 WARN DAGScheduler: Broadcasting large task binary with size 1275.0 KiB\n",
      "24/05/28 01:53:20 WARN DAGScheduler: Broadcasting large task binary with size 1287.6 KiB\n",
      "24/05/28 01:53:20 WARN DAGScheduler: Broadcasting large task binary with size 1287.6 KiB\n",
      "24/05/28 01:53:20 WARN DAGScheduler: Broadcasting large task binary with size 1287.6 KiB\n",
      "24/05/28 01:53:21 WARN DAGScheduler: Broadcasting large task binary with size 1287.6 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Composite Score: 0.8527\n",
      "Metrics - AUC: 0.8728, Accuracy: 0.8235, Precision: 0.8215, Recall: 0.8235, F1-Score: 0.8214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/28 01:53:33 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/05/28 01:53:33 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Composite Score: 0.8559\n",
      "Metrics - AUC: 0.8853, Accuracy: 0.8118, Precision: 0.8118, Recall: 0.8118, F1-Score: 0.8118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBTClassifier Composite Score: 0.8362\n",
      "Metrics - AUC: 0.8533, Accuracy: 0.8118, Precision: 0.8094, Recall: 0.8118, F1-Score: 0.8086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree Composite Score: 0.7939\n",
      "Metrics - AUC: 0.7906, Accuracy: 0.8000, Precision: 0.7974, Recall: 0.8000, F1-Score: 0.7976\n",
      "Best Model: LogisticRegression\n",
      "Best Model Composite Score: 0.8559\n",
      "Best Model Metrics - AUC: 0.8853, Accuracy: 0.8118, Precision: 0.8118, Recall: 0.8118, F1-Score: 0.8118\n",
      "Selected Regularization Parameter: 0.1\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5587f40-ecc4-4c10-b2a8-849c1a8143a4",
   "metadata": {},
   "source": [
    "The best model is LogisticRegression with metrics - AUC: 0.8853, Accuracy: 0.8118, Precision: 0.8118, Recall: 0.8118, F1-Score: 0.8118\n",
    "The selected regularization parameter was 0.1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
